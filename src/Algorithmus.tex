\section{Extraktion produktspezifischer Daten}
\label{sec:extraktion-produktspezifischer-daten}

Das Extrahieren der Informationen ist ein sehr wichtiger und kritischer Schritt, da die Genauigkeit die Ergebnisse
des Matchers und somit auch des Gesamtergebnisses stark beeinflusst.
Fehlerhafte Informationen könnten das Ergebnis so stark verfälschen, dass die Aussagekraft des Gesamtergebnisses des
Systems in Frage gestellt werden könnte.

Die größte Schwierigkeit bei der Extraktion der produktspezifischen Daten besteht darin, dass der Computer nicht
weiß, an welchen Stellen er die gewünschten Informationen findet.
Um dieses Problem zu lösen, unterscheiden wir zwischen zwei Herangehensweisen, welche im nachfolgenden Kapitel
erläutert werden.

\subsection{Die möglichen Herangehensweisen}
\label{subsec:herangehensweisen}

\begin{comment}
    schema.org
\end{comment}

Es gibt grundsätzlich zwei Möglichkeiten, wie man das Problem der Datenextraktion angehen kann.
Vordergründig geht es hierbei um die Frage, wie man aus den unterschiedlich strukturierten Shops Informationen
extrahiert.

Das von uns zu lösende Problem gibt es nicht zum ersten Mal.
Bereits Google hat sich mit der Thematik befasst, da es bei den Suchergebnissen nähere Informationen zu den Angeboten
anzeigen möchte.
Im Zuge dessen hat Google gemeinsam mit den Suchmaschinen von Microsoft, Yahoo und Yandex den Schema.org - Standard für
die strukturierte Datenangabe im Internet entwickelt.
Laut einer Schätzung von idealo verwenden rund 40\% der Shops diesen Standard.
Diese Herangehensweise bezeichnen wir als shop-unspezifischen Ansatz, da wir eine generische Regel verwenden können,
um die standardisierten Informationen zu erfassen.
Dieser Option hat den klaren Vorteil, dass sie recht schnell und einfach umzusetzen ist.

Alternativ zum shop-unspezifischen Ansatz, gibt es die shop-spezifische Herangehensweise.
Bei dem shop-spezifischen Ansatz geht es darum, für jeden Shop gesonderte Spezifikationen zu erstellen.
Die Regeln des shop-unspezifischen Ansatzes würden eine Untermenge des shop-spezifischen Ansatzes darstellen.
Die Umsetzung dieser Variante wäre insgesamt anspruchsvoller.
Insgesamt gehen wir jedoch davon aus, sowohl die Extraktionsrate als auch die Präzision des Parsers im Vergleich zu
dem shop-unspezifischen Ansatz zu erhöhen und somit bessere Daten für das Vergleichen zu erhalten.

Zu Beginn haben wir erste Versuche basierend auf dem Schema.org-Standard unternommen.
Leider haben wir schnell feststellen müssen, dass die Spezifikation oft nicht richtig eingehalten wurden, was die
Datenqualität stark verringert hat.
Auch bei anderen Standards, welche bei der Strukturierung von Produktdaten im Internet helfen sollen wie zum Beispiel
JSON-LD (W3C) und das Open-Graph-Protokoll (Facebook) konnten wir die gleiche Beobachtung machen.
Wir haben uns deshalb gegen diesen Ansatz entschieden.

Im nachfolgenden wird darauf eingegangen, wie wir die shop-spezifische Methode umgesetzt und in den Ablauf der
Extraktionsprozesses integriert haben.

\subsection{Annahmen und Grundidee}
\label{subsec:annahmen-und-grundidee}

Wir folgen der Annahme, dass jeder Shop ein CMS (Content Management System) zur Verwaltung seiner Angebote verwendet.
Des Weiteren gehen wir davon aus, dass sich durch die Verwendung eines CMS die Struktur der Angebote ähnelt und diese
erlernt werden kann.
Des Weiteren nehmen wir an, dass idealo aufgrund der Vertragsvereinbarungen für den zu untersuchenden Shops bereits
eine gewisse Menge an Angeboten besitzt und die Produktattribute nicht manipuliert wurden.

Der grobe Ablauf der shop-spezifischen Datenextraktion kann in zwei Phasen untergliedert werden:
dem Generieren der shop-spezifischen Extraktionsregeln und dem Anwenden dieser auf die von der Crawler-Komponente
heruntergeladenen Seiten.
Unter der Regel kann man sich zunächst so etwas wie eine Wegbeschreibung durch das HTML-Dokument zu dem gewünschten
Element vorstellen.
Die Grundidee der ersten Phase besteht darin, dass die Regeln, welche für das Extrahieren benötigt werden mit
Hilfe der Daten von idealo angelernt werden können.
Die generierten Regeln werden nun in der zweiten Phase angewendet, sodass für jedes Attribut genau ein Wert
zugeordnet wird.
Für jede gecrawlte Seite werden die extrahierten Werte für jedes Attribut gemeinsam abgespeichert.

Die Logik der beiden Komponenten spiegelt sich auch in der genaueren Architektur des Parsers wieder.
Dieser besteht aus der Shop Rules Generator (SRG) Komponente und der Parser Komponente.
Im nachfolgenden werde ich auf die Funktionsweise der beiden Komponenten einzeln eingehen.

\subsection{Die Funktionsweise der Parser-Komponente}
\label{subsec:funktionsweise-parser}

Die Parser-Komponente lauscht auf eine Queue, über die sie die vom Crawler heruntergeladenen Seiten erhält.
Der Crawler sendet zu jeder Seite auch die Webadresse und die Identifikationsnummer des zugehörigen Shops mit.
Nachdem der Parser etwas aus der Queue empfangen hat, fragt er den SRG nach Regeln für den entsprechenden Shop.
Sollten die Regeln noch nicht existieren, so wartet er solange, bis der SRG diese erstellt hat.
Sobald dieser die Regeln als Antwort sendet, wendet der Parser diese Regeln auf der gecrawlten Seite an.
Der extrahierte Preis und die Bilderlinks werden von dem Parser normalisiert, ehe sie gemeinsam mit den anderen
Attributen als ParsedOffer in einer Datenbank persistiert werden.
Der Matcher greift später auf diese Datenbank für den Vergleich zu.

Um den nichtfunktionalen Anforderungen, insbesondere der Skalierbarkeit und Parallelität zu genügen, arbeitet die
Parser-Komponente mit mehreren Threads und kann somit mehrere gecrawlte Seiten gleichzeitig verarbeiten.
Zudem wurden Cache-Mechanismen verwendet, um die Anfragen an den SRG zu minimieren.


\subsection{Die Funktionsweise des SRG}
\label{subsec:funktionsweise-srg}

Die Shop-Rules-Generator-Komponente wartet über eine REST-Schnittstelle auf eine Anfrage und gibt für den angegebenen
Shop die Regeln für die Extraktion zurück.
Wenn die Regel für den Shop bereits in der Datenbank existiert, wird diese vom SRG zurückgegeben.
Andernfalls wird der Generierungsprozess für den Shop gestartet.

Während des Generierungsprozesses durchläuft der SRG mehrere Phasen.
Zuerst lädt der SRG eine bestimmte Anzahl von Angeboten von der idealo-Datenbank.
Dazu nutzt er die von idealo zur Verfügung gestellte IdealoBridge - dies ist eine API, welche den Zugriff über
eine REST-Schnittstelle kapselt.
Zu jedem dieser Angebote liegen die Adresse für das Angebot, sowie die  Informationen über die in
Kapitel~\ref{subsec:technische-anforderungen-parser} genannten Attribute vor.
Für jedes Angebot ruft der SRG die verlinkte Angebotsseite auf und speichert das HTML-Dokument dieser.
Um die Internetshops nicht mit zu vielen Anfragen in zu kurzer Zeit zu belasten, wird zwischen jeder Angebotsseite
eine fest definierte Zeit gewartet.
Wir wissen nun, dass es sich bei dem HTML-Dokument um das bestimmte Angebot handelt und können dies ausnutzen.
Dazu sucht der SRG den Wert aller einzelnen Produktattribute und erstellt für jeden Fundort eine Regel, welche auf
das entsprechende Element zeigt.
Nachdem er dies für alle heruntergeladenen Angebote gemacht hat, kann er aus den gesammelten Regeln eine
finale Regelmenge bestimmen.
Diese Regelmenge wird in einer Datenbank gespeichert und bei zukünftigen Anfragen direkt zurückgegeben.

\subsection{Der URL-Cleaner}
\label{subsec:urlcleaner}

Manche Onlinehändler manipulieren vor dem Übermitteln der Daten an idealo die Links zu den Angeboten, indem sie sie
mit Klick-Trackern versehen.
Mit Hilfe von Klick-Trackern können Shops beispielsweise nachvollziehen, wie oft Kunden über idealo auf deren Seite
gelangt sind und stellen ein Kontrollwerkzeug für die CPC-Abrechnung dar.
Um die Statistik der Shops nicht durch die Besuche des SRGs zu verfälschen, ist es idealo wichtig, dass wir die
Angebotsadressen vor dem Aufruf von den Klickinformationen bereinigen, insofern dies möglich ist.
Dazu haben wir die URL-Cleaner-Komponente entwickelt, welche Adressen mit Klicktrackern als Eingabe erwartet und
bereinigt zurück gibt.

Für die Bereinigung werden zwei Strategien nacheinander ausgeführt.
Die erste ist für die Bereinigung von Redirects verantwortlich und die zweite für das Bereinigen von Tracker-Parametern.
Nachdem beide Verfahren angewandt wurden, wird die bereinigte URL zurück gegeben.

Redirects zeichnen sich dadurch aus, dass der Link auf einen anderen Host verweist und die Adresse erst mit dem
Besuch zu tatsächlichen Adresse aufgelöst wird.
Dabei gibt es zwei mögliche Verfahren der Redirect-Dienste:
Beim ersten Verfahren wird einfach der mitgesendete URL-Parameter encodiert als Redirect gesetzt.
Das zweite Verfahren ist etwas schwieriger, da es eine Datenbank für die Auflösung eines mitgesendeten kryptischen
Identifikationstrings verwendet, um den Redirect aufzulösen.

Für das erste Szenario encodiert der URL-Cleaner die übergebene URL und entfernt alles, was vor der Root-URL des
Shops steht.
Mit der zweiten Variante kann der URL-Cleaner nicht umgehen, kann dies aber - da die Root-URL des Shops nicht
enthalten ist, erkennen und eine entsprechende Meldung zurückgeben.

Um die parameter-basierten Trackerinformationen zu entfernen, löscht der URL-Cleaner alle
Schlüssel-Wert-Parameterkombinationen, deren Schlüssel in einer Liste vorkommen.
Diese Liste enthält alle Schlüsselnamen, die uns bei einer manuellen Recherche über mehrere hundert Shops mehrmals
vorgekommen sind.

Beispiel für Redirect
Beispiel für Tracker-Parametern