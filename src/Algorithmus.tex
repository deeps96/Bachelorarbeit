\section{Extraktion produktspezifischer Daten}
\label{sec:extraktion-produktspezifischer-daten}

Das Extrahieren der Informationen ist ein sehr wichtiger und kritischer Schritt, da die Genauigkeit die Ergebnisse
des Matchers und somit auch des Gesamtergebnisses stark beeinflusst.
Fehlerhafte Informationen könnten das Ergebnis so stark verfälschen, dass die Aussagekraft des Gesamtergebnisses des
Systems in Frage gestellt werden könnte.

Die größte Schwierigkeit bei der Extraktion der produktspezifischen Daten besteht darin, dass der Computer nicht
weiß, an welchen Stellen er die gewünschten Informationen findet.
Um dieses Problem zu lösen, unterscheiden wir zwischen zwei Herangehensweisen, welche im nachfolgenden Kapitel
erläutert werden.

\subsection{Die möglichen Herangehensweisen}
\label{subsec:herangehensweisen}

\begin{comment}
    schema.org
\end{comment}

Es gibt grundsätzlich zwei Möglichkeiten, wie man das Problem der Datenextraktion angehen kann.
Vordergründig geht es hierbei um die Frage, wie man aus den unterschiedlich strukturierten Shops Informationen
extrahiert.

Das von uns zu lösende Problem gibt es nicht zum ersten Mal.
Bereits Google hat sich mit der Thematik befasst, da es bei den Suchergebnissen nähere Informationen zu den Angeboten
anzeigen möchte.
Im Zuge dessen hat Google gemeinsam mit den Suchmaschinen von Microsoft, Yahoo und Yandex den Schema.org - Standard für
die strukturierte Datenangabe im Internet entwickelt.
Laut einer Schätzung von idealo verwenden rund 40\% der Shops diesen Standard.
Diese Herangehensweise bezeichnen wir als shop-unspezifischen Ansatz, da wir eine generische Regel verwenden können,
um die standardisierten Informationen zu erfassen.
Dieser Option hat den klaren Vorteil, dass sie recht schnell und einfach umzusetzen ist.

Alternativ zum shop-unspezifischen Ansatz, gibt es die shop-spezifische Herangehensweise.
Bei dem shop-spezifischen Ansatz geht es darum, für jeden Shop gesonderte Spezifikationen zu erstellen.
Die Regeln des shop-unspezifischen Ansatzes würden eine Untermenge des shop-spezifischen Ansatzes darstellen.
Die Umsetzung dieser Variante wäre insgesamt anspruchsvoller.
Insgesamt gehen wir jedoch davon aus, sowohl die Extraktionsrate als auch die Präzision des Parsers im Vergleich zu
dem shop-unspezifischen Ansatz zu erhöhen und somit bessere Daten für das Vergleichen zu erhalten.

Zu Beginn haben wir erste Versuche basierend auf dem Schema.org-Standard unternommen.
Leider haben wir schnell feststellen müssen, dass die Spezifikation oft nicht richtig eingehalten wurden, was die
Datenqualität stark verringert hat.
Auch bei anderen Standards, welche bei der Strukturierung von Produktdaten im Internet helfen sollen wie zum Beispiel
JSON-LD (W3C) und das Open-Graph-Protokoll (Facebook) konnten wir die gleiche Beobachtung machen.
Wir haben uns deshalb gegen diesen Ansatz entschieden.

Im nachfolgenden wird darauf eingegangen, wie wir die shop-spezifische Methode umgesetzt und in den Ablauf der
Extraktionsprozesses integriert haben.

\subsection{Annahmen und Grundidee}
\label{subsec:annahmen-und-grundidee}

Wir folgen der Annahme, dass jeder Shop ein CMS (Content Management System) zur Verwaltung seiner Angebote verwendet.
Des Weiteren gehen wir davon aus, dass sich durch die Verwendung eines CMS die Struktur der Angebote ähnelt und diese
erlernt werden kann.
Des Weiteren nehmen wir an, dass idealo aufgrund der Vertragsvereinbarungen für den zu untersuchenden Shops bereits
eine gewisse Menge an Angeboten besitzt und die Produktattribute nicht manipuliert wurden.

Der grobe Ablauf der shop-spezifischen Datenextraktion kann in zwei Phasen untergliedert werden:
dem Generieren der shop-spezifischen Extraktionsregeln und dem Anwenden dieser auf die von der Crawler-Komponente
heruntergeladenen Seiten.
Die Grundidee der ersten Phase besteht darin, dass die Regeln, welche für das Extrahieren benötigt werden mit
Hilfe der Daten von idealo angelernt werden können.
Dazu lädt der Parser eine bestimmte Anzahl von Angeboten aus dem bereits bestehenden idealo-Katalog.
Zu jedem dieser Angebote liegen die Adresse für das Angebot, sowie die  Informationen über die in
Kapitel\ref{subsec:technische-anforderungen-parser} genannten Attribute vor.
Der Parser lädt die verlinkten Angebotsseiten herunter und kann nun ausnutzen, dass er weiß, welche Attribute in dem
heruntergeladenem HTML-Dokument vorhanden sein müssen.
Dazu sucht er den den Wert aller einzelnen Produktattribute und kann sich für jedes Vorkommnis eine Regel erstellen.
Unter der Regel kann man sich zunächst wie eine Wegbeschreibung durch das HTML-Dokument zu dem gewünschten Element
vorstellen.
Nachdem der Parser dies für alle heruntergeladenen Angebote gemacht hat, kann er aus den gesammelten Regeln eine
finale Regelmenge bestimmen.

Die generierten Regeln werden nun in der zweiten Phase angewendet, sodass für jedes Attribut genau ein Wert
zugeordnet wird.
Für jede gecrawlte Seite werden die extrahierten Werte für jedes Attribut gemeinsam abgespeichert.

\subsection{Die Funktionsweise}
\label{subsec:funktionsweise}