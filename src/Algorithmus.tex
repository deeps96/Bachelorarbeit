\section{Extraktion produktspezifischer Daten}
\label{sec:extraktion-produktspezifischer-daten}

% TODO
% Kurze Einleitung für das Kapitel schreiben und sagen worum es geht und warum Parser wichtig ist.
Placeholder \\
Placeholder \\
Placeholder \\
Placeholder

\subsection{Die technischen Anforderungen an den Parser}
\label{subsec:technische-anforderungen-parser}

Die Herausforderung der Parser-Komponente besteht hauptsächlich darin, das heterogene Informationsschemata der
verschiedenen Shops in ein homogenes, normalisierten Schema zu bringen.
Im Detail geht es darum, zu jedem Angebot den Titel, die Produktbeschreibung, den Preis, die Marke, die Kategorie,
die Produktbilder sowie weitere eindeutige Merkmale im Format von idealo zu erfassen.
Diese eindeutigen Merkmale sind zum Beispiel die standardisierte EAN (Europäische Artikelnummer), HAN (Händler
Artikelnummer) und SKU (Stock keeping unit/ eine shop-spezifische Kennung).

Da die Crawler-Komponente sehr viel Zeit benötigt um alle Seiten zu erfassen, spielt der Zeitfaktor für den Parser
keine große Rolle.
Eine schnelle Verarbeitung der Seiten ist dennoch wünschenswert.
Damit die Ergebnisse des Parsers als zuverlässig eingestuft werden und vom Matcher weiterverwenden werten können, ist
eine hohe Präzision und eine hohe Extraktionsrate erforderlich.

Die eigentliche Schwierigkeit der Datenextraktion liegt in dem Bestimmten der Stellen, an denen die gewünschten
Informationen stehen.
Um dieses Problem zu lösen, haben wir zwischen zwei Herangehensweisen unterschieden.
Diese werden im nachfolgenden Kapitel erläutert.

\subsection{Die möglichen Herangehensweisen}
\label{subsec:herangehensweisen}

\begin{comment}
    schema.org
\end{comment}
Es gibt grundsätzlich zwei Möglichkeiten, wie man aus den Internetangeboten Informationen extrahieren kann.
Wir haben zwischen dem shop-unspezifischen und den shop-spezifischen Ansatz unterschieden.

Den shop-unspezifischen Ansatz hat bereits die Initiative Schema.org aufgegriffen.
Schema.org hat einen Standard entwickelt, den Webseitenbetreiber nutzen können, um bestimmte
Daten zu markieren.
Shopbetreiber können zum Beispiel die Produktrezensionen, den Preis oder auch den Produkttitel hervorheben.
Große Suchmaschinenanbieter wie Google, Microsoft oder Yandex können dadurch sehr einfach die relevanten Informationen
direkt in den Suchergebnissen anzeigen.
Die Angebote der Onlinehändler werden somit besser dargestellt.

Laut einer Schätzung von idealo verwenden rund 40\% der Shops diesen Standard.
Diese Herangehensweise bezeichnen wir als shop-unspezifischen Ansatz, da man generische Regeln verwenden kann,
um die standardisierten Informationen zu erfassen.
Die Lösung ist recht einfach und schnell umsetzbar.

Alternativ zum shop-unspezifischen Ansatz gibt es die shop-spezifische Herangehensweise, d.h.\ dass für jeden
Onlineshop individuell angepasste Spezifikationen für die Extraktion erstellt werden.
Die Regeln des shop-spezifischen Ansatzes bilden eine Übermenge des shop-unspezifischen Ansatzes.

Die Umsetzung dieser Variante ist insgesamt anspruchsvoller, da diese Spezifikationen zunächst erstellt werden müssten.
Insgesamt gehen wir jedoch davon aus, durch diesen Ansatz sowohl die Extraktionsrate als auch die Präzision des Parsers
im Vergleich zu dem shop-unspezifischen Ansatz zu erhöhen.
Dadurch erhoffen wir uns bessere Daten für den Vergleich zu erhalten.

Zu Beginn haben wir erste Versuche basierend auf dem Schema.org-Standard unternommen.
Leider haben wir schnell feststellen müssen, dass die Spezifikation oft nicht richtig eingehalten wurden.
Dies hat die Qualität der extrahierten Daten stark verringert.
Auch bei anderen Standards, welche bei der Strukturierung von Produktdaten im Internet helfen sollen, wie zum Beispiel
JSON-LD (W3C) und das Open-Graph-Protokoll (Facebook), konnten wir ähnliche Beobachtungen machen.
Wir haben uns deshalb gegen den shop-unspezifischen Ansatz entschieden.

Im nachfolgenden wird darauf eingegangen, wie wir die shop-spezifische Methode umgesetzt und in den Ablauf des
Extraktionsprozesses integriert haben.

\subsection{Annahmen}
\label{subsec:annahmen}

Für die Umsetzung der Parser-Komponente haben wir zwei Annahmen getroffen, welche die Konzeption des Algorithmus
beeinflusst haben.
Wir nehmen an, dass jeder Shop ein CMS (Content Management System) zur Verwaltung seiner Angebote verwendet.
Daraus resultierend gehen wir davon aus, dass sich durch die Verwendung eines CMS die Struktur der Angebote eines Shops
ähnelt und diese erlernt werden kann.
Des Weiteren nehmen wir an, dass idealo aufgrund der Vertragsvereinbarungen für die zu untersuchenden Shops bereits
eine gewisse Menge an Angeboten besitzt und die Produktattribute nicht manipuliert wurden.

\subsection{Die Grundidee}
\label{subsec:grundidee}

Der grobe Ablauf der shop-spezifischen Datenextraktion kann in zwei Phasen untergliedert werden:
\begin{enumerate}
    \item Die Generierung der shop-spezifischen Extraktionsregeln/ Spezifikation
    \item Das Anwenden der Regeln auf die vom Crawler erzeugten Seiten
\end{enumerate}
Eine Regel ist eine Sammlung von Selektoren für eine bestimme Produkteigenschaft.
Ein Selektor ist eine Art Wegbeschreibung durch das HTML-Dokument.
Er führt zu dem gewünschten Element, aus dem das Produktattribut extrahiert werden soll.
In der ersten Phase sollen die Regeln, welche für das Extrahieren benötigt werden, mit Hilfe der Daten von idealo
angelernt werden.
Die generierten Regeln werden in der zweiten Phase angewendet, sodass für jede Produkteigenschaft genau ein Wert
zugeordnet wird.
Für jede gecrawlte Seite werden die extrahierten Produktattribute abgespeichert.

Die Logik der beiden Phasen spiegelt sich in der Architektur des Parsers wieder.
Dieser besteht aus dem Shop Rules Generator (SRG), der Parser-Komponente und dem URL-Cleaner.
Auf die Notwendigkeit des URL-Cleaners und dessen Funktionsweise wird in Kapitel~\ref{subsec:urlcleaner} eingegangen.
Die resultierende Architektur ist in Abbildung~\ref{abb:architektur-parser} abgebildet.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, trim=0 1.7cm 0 1.7cm, clip]{resources/Architektur-Parser.pdf}
    \caption{Architektur des Parsers}
    \label{abb:architektur-parser}
\end{figure}

\subsection{Die Funktionsweise der Parser-Komponente}
\label{subsec:funktionsweise-parser}

Die Parser-Komponente erhält ihre Eingaben, indem sie Nachrichten aus einer Queue konsumiert.
Eine Nachricht enthält eine vom Crawler heruntergeladene Webseite.
Der Crawler sendet zusätzlich zu jeder Seite die Webadresse und die Identifikationsnummer des zugehörigen Shops.
Nach dem Erhalt einer Nachricht, lädt der Parser vom SRG die Extraktionsregeln für den entsprechenden Shop.
Sollten die Regeln noch nicht existieren, wartet der Parser solange, bis diese vom SRG erstellt wurden.
Sobald der Parser die Regeln empfangen hat, werden die Produktattribute aus der gecrawlten Seite extrahiert.
Die extrahierten Daten werden abschließend in einer Datenbank normalisiert gespeichert.

Der Matcher greift später auf diese Datenbank für den Vergleich zu.

\subsection{Die Funktionsweise des SRG}
\label{subsec:funktionsweise-srg}

Der Shop Rules Generator gibt auf Anfrage die Regeln für einen beliebigen Shop zurück.
Sollten diese noch nicht existieren, werden sie generiert.

Während des Generierungsprozesses durchläuft der SRG mehrere Schritte.
Zuerst wird eine bestimmte Anzahl von Angeboten aus der idealo-Datenbank geladen.
Dies erfolgt über einen von idealo zur Verfügung gestellten API-Endpoint, welcher den Datenbankzugriff über eine
REST-Schnittstelle kapselt.
Dies hat den Vorteil, dass wir die Infrastruktur von idealo nutzen können und keine Kopie der Angebotsdaten
lokal speichern müssen.

Zu jedem dieser Angebote liegen die Webadresse für das Angebot, sowie die  Informationen über die in
Kapitel~\ref{subsec:technische-anforderungen-parser} genannten Produktattribute vor.
Außerdem wird für jedes Angebot das dazugehörige HTML-Dokument heruntergeladen.

Um die Server der Onlineshops nicht mit zu vielen Anfragen auf einmal zu strapazieren, wird zwischen jedem
Herunterladen einer Angebotsseite eine fest definierte Zeit gewartet.

Für die Menge der heruntergeladenen HTML-Dokumente ist bekannt, um welche Angebote es sich handelt und welche
konkreten Produktattribute erwartet werden.
Dieses Wissen wird für das Anlernen der Regeln genutzt.
Dazu werden die Werte aller Produkteigenschaften in dem HTML-Dokument gesucht.
Für jedes Vorkommnis eines Produktattributes wird ein Selektor erstellt, der den Fundort referenziert und einer Regel
zugeordnet.
Nachdem alle Regeln gesammelt wurden, wird eine finale Regelmenge bestimmt.
In dieser Regelmenge ist für jede Produkteigenschaft eine Menge von Selektoren enthalten.
Diese Regelmenge wird in der Regeldatenbank gespeichert und bei zukünftigen Anfragen direkt zurückgegeben.

\subsection{Der URL-Cleaner}
\label{subsec:urlcleaner}

Manche Onlinehändler manipulieren vor der Übermittlung ihrer Angebote an idealo die Links zu deren Angeboten.
Sie fügen zu den regulären Webadressen Trackinginformationen hinzu.
Mit Hilfe dieser Trackinginformationen können sie nachvollziehen, welche Kunden durch idealo auf deren Seite gelandet
sind.
Dies ist für die Shopbesitzer sehr wichtig, da sie somit die CPC-Abrechnung von idealo kontrollieren können.
Die im Rahmen der Anlernphase getätigten Webseitenaufrufe könnten diese Statistiken jedoch verfälschen.
Für idealo ist es deshalb sehr wichtig, dass die Trackinginformationen vor dem Aufruf der Website entfernt werden.

Dazu haben wir die URL-Cleaner-Komponente entwickelt, welche Adressen mit Trackinginformationen als Eingabe erwartet und
bereinigt zurück gibt.

Wir haben zwei mögliche Verfahren entdeckt, wie die Trackinginformationen in Angebotslinks eingefügt werden können.
Dies ist sowohl über Redirects als auch URL-Parameter möglich.
Der URL-Cleaner wendet deshalb zwei Strategien sukzessive an, um die Trackinginformationen zu entfernen:

Die erste Strategie bereinigt die URL von Redirects.
Dabei gibt es zwei mögliche Verfahren der Redirect-Dienste:
Beim ersten Verfahren leitet der zwischengeschaltete Server den Besucher anhand der mitgesendeten URL-Parameter auf
die korrekte Seite weiter.
Die ursprüngliche URL ist hierbei somit bereits in der Redirect-URL enthalten.
Das zweite Verfahren ist etwas schwieriger, da anstatt der Ziel-Adresse lediglich ein kryptischer String mitgesendet
wird.
Die Weiterleitung ist erst nach der serverseitigen Zuordnung des kryptischen Strings zur tatsächlichen Adresse möglich.

In der Tabelle~\ref{tab:redirect} sind zwei Beispiele abgebildet, wie solche Redirect-Links aussehen könnten.
Eine Mischform der beiden Varianten ist ebenfalls möglich.

\begin{table}[h]
    \centering
    \begin{tabular}{ l | l }
        Redirect mit URL-Parameter                  &   cptrack.com/?redir=www.shop.de/product1\\
        Redirect mit krypt.\ Identifikationsstring   &   bit.ly/2Kqyrz2
    \end{tabular}
    \caption{Beispiele der Redirect-Verfahren}
    \label{tab:redirect}
\end{table}

Für das erste Redirect-Szenario wird die übergebene URL zunächst encodiert.
Anschließend wird die Root-Url des Shops in der encodierten URL gesucht und alles davor entfernt.
Für das zweite Szenario haben wir keine zufriedenstellende Lösung finden können.
Allerdings erkennt der URL-Cleaner jedoch, dass er mit diesem Fall nicht umgehen kann, da die Root-URL des Shops
nicht in der encodierten URL enthalten ist.

\begin{table}[h]
    \centering
    \begin{tabular}{ c }
        http://www.shop.de/product?\textbf{partner=idealo}?pid=96
    \end{tabular}
    \caption{URL mit parameterbasierten Trackinginformationen}
    \label{tab:trackparameter}
\end{table}

Die Tabelle~\ref{tab:trackparameter} zeigt eine Beispiel-URL mit parameterbasierten Trackerinformationen.
\textit{(Diese sind fett hervorgehoben.)}
Dabei können ebenfalls Parameter enthalten sein, welche nicht für das Tracking verwendet werden.

Um die parameterbasierten Trackerinformationen zu entfernen, werden alle Schlüssel-Wert-Parameterkombinationen
gelöscht, deren Schlüssel in einer vorher angelegten Liste vorkommen.
Diese Liste enthält alle Tracker-Schlüsselnamen, die bei einer manuellen Recherche über mehrere hundert Shops
häufiger vorgekommen sind.

Nachdem beide Strategien angewandt wurden, wird die bereinigte URL zurückgegeben.

\subsection{Das Erstellen von Selektoren}
\label{subsec:erstellen-von-selektoren}

Im nachfolgenden wird darauf eingegangen, wie die Selektoren während der Regelgenerierung erzeugt werden.
Für die Erläuterung werden Begriffe wie \textit{Document Object Model (DOM)}, \textit{Element} oder \textit{Tag} aus
dem Kontext der hierarchiebasierten Sprache HTML verwendet.
Bei Bedarf kann der entsprechende Wikipedia-Artikel\footnotemark für einen kurzen Einstieg genutzt werden.
\footnotetext{\url{https://de.wikipedia.org/wiki/Hypertext_Markup_Language}}

Um einen Selektor zu erstellen, muss zunächst ein konkretes Element der DOM-Hierarchie bestimmt werden.
Dieses wird von dem SRG in einem vorherigen Schritt ermittelt und stellt den Fundort für ein gewünschtes
Produktattribut dar.

Es wird zwischen den folgenden drei Knotentypen unterschieden: Textknoten, Beschreibungsknoten und Datenknoten.

\textit{Textknoten} sind Elemente, bei denen das gewünschte Produktattribut innerhalb eines Tag-Paars steht.
Das Attribut ist somit ein sichtbarer Bestandteil der Browservisualisierung.

Zu den \textit{Beschreibungsknoten} gehören die Elemente, bei denen das gesuchte Produktattribut innerhalb der
Attributliste des Elementes vorkommt.
Dieses Attribut ist im Gegensatz zum Textknoten kein sichtbarer Bestandteil der Visualisierung.

In dem Beispiel~\ref{bsp:text-and-beschreibungsknoten} ist jeweils ein kurzes Beispiel für beide Typen aufgeführt.
Der gesuchte Wert ist in diesem Fall die Produkteigenschaft EAN mit dem Produktattribut 9332721000108.

\lstset{
    frame = single,
    language=html,
}
\begin{lstlisting}[
    caption=Beispiele für einen Textknoten und einen Beschreibungsknoten,
    captionpos=b,
    label=bsp:text-and-beschreibungsknoten]
    <div id='product-details'>
    <span>9332721000108</span>
    </div>
    <meta itemprop='gtin13' content='9332721000108'>
\end{lstlisting}

Die Selektoren der beiden Knotentypen sind ähnlich aufgebaut und bestehen jeweils aus einem CSS-Selektor.
Dieser CSS-Selektor ist minimal lang und eindeutig, das heißt dass der Selektor nur auf exakt ein Element innerhalb
des Modells zeigen kann.
Der Aufbau des CSS-Selektors erfolgt dabei ähnlich der wie bei der "Copy selector"-Funktion der
Chrome-Entwicklertools und wird beim traversieren vom Element zur Wurzel des Dokumentes erzeugt.
Der Selektor für einen Beschreibungsknoten speichert zusätzlich den Schlüssel, um das korrekte Elementattribut
auszulesen.

% TODO Datenselektor erklären und Erstellung erläutern