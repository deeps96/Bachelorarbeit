\section{Extraktion produktspezifischer Daten}
\label{sec:extraktion-produktspezifischer-daten}

Das Extrahieren der produktspezifischen Informationen ist ein sehr wichtiger und kritischer Schritt des gesamten
Softwaresystems.
Die Genauigkeit der Parser-Komponente ist maßgeblich für die Qualität der extrahierten Daten.
Die Ergebnisse des Matchers sind abhängig von denen der Parser-Komponente, der Parser hat somit einen starken
Einfluss auf die Ergebnisse des Matchers und somit auch des Gesamtergebnisses des Systems.
Sollten die Extraktion stark fehlerbehaftet sein, könnte dessen Aussagekraft in Frage gestellt werden.

Die eigentliche Schwierigkeit der Datenextraktion liegt in dem bestimmten der Stellen, an denen die gewünschten
Informationen stehen.
Um dieses Problem zu lösen, haben wir zwischen zwei Herangehensweisen unterschieden.
Diese werden im nachfolgenden Kapitel erläutert.

\subsection{Die möglichen Herangehensweisen}
\label{subsec:herangehensweisen}

\begin{comment}
    schema.org
\end{comment}

Es gibt grundsätzlich zwei Möglichkeiten, wie man das Problem der Datenextraktion angehen kann.
Vordergründig geht es hierbei um die Frage, wie man aus den unterschiedlich strukturierten Shops Informationen
extrahiert.

Das von uns zu lösende Problem gibt es nicht zum ersten Mal.
Bereits Google hat sich mit der Thematik befasst, da es bei den Suchergebnissen nähere Informationen zu den Angeboten
anzeigen möchte.
Im Zuge dessen hat Google gemeinsam mit den Suchmaschinen von Microsoft, Yahoo und Yandex den Schema.org - Standard für
die strukturierte Datenangabe im Internet entwickelt.
Laut einer Schätzung von idealo verwenden rund 40\% der Shops diesen Standard.
Diese Herangehensweise bezeichnen wir als shop-unspezifischen Ansatz, da wir eine generische Regel verwenden können,
um die standardisierten Informationen zu erfassen.
Dieser Option hat den klaren Vorteil, dass sie recht schnell und einfach umzusetzen ist.

Alternativ zum shop-unspezifischen Ansatz, gibt es die shop-spezifische Herangehensweise.
Bei dem shop-spezifischen Ansatz geht es darum, für jeden Shop gesonderte Spezifikationen zu erstellen.
Die Regeln des shop-unspezifischen Ansatzes würden eine Untermenge des shop-spezifischen Ansatzes darstellen.
Die Umsetzung dieser Variante wäre insgesamt anspruchsvoller.
Insgesamt gehen wir jedoch davon aus, sowohl die Extraktionsrate als auch die Präzision des Parsers im Vergleich zu
dem shop-unspezifischen Ansatz zu erhöhen und somit bessere Daten für das Vergleichen zu erhalten.

Zu Beginn haben wir erste Versuche basierend auf dem Schema.org-Standard unternommen.
Leider haben wir schnell feststellen müssen, dass die Spezifikation oft nicht richtig eingehalten wurden, was die
Datenqualität stark verringert hat.
Auch bei anderen Standards, welche bei der Strukturierung von Produktdaten im Internet helfen sollen wie zum Beispiel
JSON-LD (W3C) und das Open-Graph-Protokoll (Facebook) konnten wir die gleiche Beobachtung machen.
Wir haben uns deshalb gegen diesen Ansatz entschieden.

Im nachfolgenden wird darauf eingegangen, wie wir die shop-spezifische Methode umgesetzt und in den Ablauf der
Extraktionsprozesses integriert haben.

\subsection{Annahmen und Grundidee}
\label{subsec:annahmen-und-grundidee}

Wir folgen der Annahme, dass jeder Shop ein CMS (Content Management System) zur Verwaltung seiner Angebote verwendet.
Des Weiteren gehen wir davon aus, dass sich durch die Verwendung eines CMS die Struktur der Angebote ähnelt und diese
erlernt werden kann.
Des Weiteren nehmen wir an, dass idealo aufgrund der Vertragsvereinbarungen für den zu untersuchenden Shops bereits
eine gewisse Menge an Angeboten besitzt und die Produktattribute nicht manipuliert wurden.

Der grobe Ablauf der shop-spezifischen Datenextraktion kann in zwei Phasen untergliedert werden:
dem Generieren der shop-spezifischen Extraktionsregeln und dem Anwenden dieser auf die von der Crawler-Komponente
heruntergeladenen Seiten.
Unter der Regel kann man sich zunächst so etwas wie eine Wegbeschreibung durch das HTML-Dokument zu dem gewünschten
Element vorstellen.
Die Grundidee der ersten Phase besteht darin, dass die Regeln, welche für das Extrahieren benötigt werden mit
Hilfe der Daten von idealo angelernt werden können.
Die generierten Regeln werden nun in der zweiten Phase angewendet, sodass für jedes Attribut genau ein Wert
zugeordnet wird.
Für jede gecrawlte Seite werden die extrahierten Werte für jedes Attribut gemeinsam abgespeichert.

Die Logik der beiden Komponenten spiegelt sich auch in der genaueren Architektur des Parsers wieder.
Dieser besteht aus der Shop Rules Generator (SRG) Komponente und der Parser Komponente.
Im nachfolgenden werde ich auf die Funktionsweise der beiden Komponenten einzeln eingehen.

\subsection{Die Funktionsweise der Parser-Komponente}
\label{subsec:funktionsweise-parser}

Die Parser-Komponente lauscht auf eine Queue, über die sie die vom Crawler heruntergeladenen Seiten erhält.
Der Crawler sendet zu jeder Seite auch die Webadresse und die Identifikationsnummer des zugehörigen Shops mit.
Nachdem der Parser etwas aus der Queue empfangen hat, fragt er den SRG nach Regeln für den entsprechenden Shop.
Sollten die Regeln noch nicht existieren, so wartet er solange, bis der SRG diese erstellt hat.
Sobald dieser die Regeln als Antwort sendet, wendet der Parser diese Regeln auf der gecrawlten Seite an.
Der extrahierte Preis und die Bilderlinks werden von dem Parser normalisiert, ehe sie gemeinsam mit den anderen
Attributen als ParsedOffer in einer Datenbank persistiert werden.
Der Matcher greift später auf diese Datenbank für den Vergleich zu.

Um den nichtfunktionalen Anforderungen, insbesondere der Skalierbarkeit und Parallelität zu genügen, arbeitet die
Parser-Komponente mit mehreren Threads und kann somit mehrere gecrawlte Seiten gleichzeitig verarbeiten.
Zudem wurden Cache-Mechanismen verwendet, um die Anfragen an den SRG zu minimieren.


\subsection{Die Funktionsweise des SRG}
\label{subsec:funktionsweise-srg}

Die Shop-Rules-Generator-Komponente wartet über eine REST-Schnittstelle auf eine Anfrage und gibt für den angegebenen
Shop die Regeln für die Extraktion zurück.
Wenn die Regel für den Shop bereits in der Datenbank existiert, wird diese vom SRG zurückgegeben.
Andernfalls wird der Generierungsprozess für den Shop gestartet.

Während des Generierungsprozesses durchläuft der SRG mehrere Phasen.
Zuerst lädt der SRG eine bestimmte Anzahl von Angeboten von der idealo-Datenbank.
Dazu nutzt er die von idealo zur Verfügung gestellte IdealoBridge - dies ist eine API, welche den Zugriff über
eine REST-Schnittstelle kapselt.
Zu jedem dieser Angebote liegen die Adresse für das Angebot, sowie die  Informationen über die in
Kapitel~\ref{subsec:technische-anforderungen-parser} genannten Attribute vor.
Für jedes Angebot ruft der SRG die verlinkte Angebotsseite auf und speichert das HTML-Dokument dieser.
Um die Internetshops nicht mit zu vielen Anfragen in zu kurzer Zeit zu belasten, wird zwischen jeder Angebotsseite
eine fest definierte Zeit gewartet.
Wir wissen nun, dass es sich bei dem HTML-Dokument um das bestimmte Angebot handelt und können dies ausnutzen.
Dazu sucht der SRG den Wert aller einzelnen Produktattribute in dem HTML-Dokument und erstellt für jeden Fundort eine
Regel, welche auf das entsprechende Element zeigt.
Nachdem er dies für alle heruntergeladenen Angebote gemacht hat, kann er aus den gesammelten Regeln eine
finale Regelmenge bestimmen.
Diese Regelmenge wird in einer Datenbank gespeichert und bei zukünftigen Anfragen direkt zurückgegeben.

\subsection{Der URL-Cleaner}
\label{subsec:urlcleaner}

Manche Onlinehändler manipulieren vor dem Übermitteln der Daten an idealo die Links zu den Angeboten, indem sie sie
mit Klick-Trackern versehen.
Mit Hilfe von Klick-Trackern können Shops beispielsweise nachvollziehen, wie oft Kunden über idealo auf deren Seite
gelangt sind und stellen ein Kontrollwerkzeug für die CPC-Abrechnung dar.
Um die Statistik der Shops nicht durch die Besuche des SRGs zu verfälschen, ist es idealo wichtig, dass wir die
Angebotsadressen vor dem Aufruf von den Klickinformationen bereinigen, insofern dies möglich ist.
Dazu haben wir die URL-Cleaner-Komponente entwickelt, welche Adressen mit Klicktrackern als Eingabe erwartet und
bereinigt zurück gibt.

Für die Bereinigung werden zwei Strategien nacheinander ausgeführt.
Die erste ist für die Bereinigung von Redirects verantwortlich und die zweite für das Bereinigen von Tracker-Parametern.
Nachdem beide Verfahren angewandt wurden, wird die bereinigte URL zurück gegeben.

Redirects zeichnen sich dadurch aus, dass der Link auf einen anderen Host verweist und die Adresse erst mit dem
Besuch zu tatsächlichen Adresse aufgelöst wird.
Dabei gibt es zwei mögliche Verfahren der Redirect-Dienste:
Beim ersten Verfahren wird einfach der mitgesendete URL-Parameter encodiert als Redirect gesetzt.
Das zweite Verfahren ist etwas schwieriger, da es eine Datenbank für die Auflösung eines mitgesendeten kryptischen
Identifikationsstrings verwendet, um den Redirect aufzulösen.

Für das erste Szenario encodiert der URL-Cleaner die übergebene URL und entfernt alles, was vor der Root-URL des
Shops steht.
Mit der zweiten Variante kann der URL-Cleaner nicht umgehen, kann dies aber - da die Root-URL des Shops nicht
enthalten ist, erkennen und eine entsprechende Meldung zurückgeben.

Um die parameter-basierten Trackerinformationen zu entfernen, löscht der URL-Cleaner alle
Schlüssel-Wert-Parameterkombinationen, deren Schlüssel in einer bestimmten Liste vorkommen.
Diese Liste enthält alle Schlüsselnamen, die uns bei einer manuellen Recherche über mehrere hundert Shops häufiger
vorgekommen sind.

Beispiel für Redirect
Beispiel für Tracker-Parametern

\subsection{Das Erstellen von Selektoren}
\label{subsec:erstellen-von-selektoren}

Nachdem nun die allgemeine Vorgehensweise des Parsers und des Shop Rules Generators beschrieben wurden, geht es nun
konkret um das Erstellen von Selektoren.
Ein Selektor ist dabei wie eine Art Pfad und zeigt auf ein Element innerhalb der hierarchischen HTML-Struktur.
Wir kategorisieren die HTML-Elemente anhand dessen, wo das gesuchte Produktattribut steht.
Jedes HTML-Element wird durch ein Tag beschrieben.
Es gibt nun drei mögliche Orte, an denen das Produktattribut stehen kann: innerhalb eines Tags, in der Tag-Beschreibung
oder im Javascript.
Im nachfolgenden wird anhand eines Beispiels erklärt, wie die Regeln für jeden Typ erstellt werden.
Die EAN ist ein wichtiges Attribut, da es das Matching vereinfacht.
Wir wollen daher für unsere Beispiele jeweils einen Selektor für die EAN 9332721000108 erstellen.

\subsubsection{Das Erstellen von Text-Selektoren}
\label{subsubsec:erstellen-von-text-selektoren}

Wenn der gesuchte Wert in der Browser-Visualisierung der HTML-Seite enthalten ist, so lässt sich dafür ein
Text-Selektor bauen.
Dieser Selektor besteht zunächst lediglich aus einem eindeutigen CSS-Selektor, welcher minimal lang ist.
Eindeutig in diesem Kontext bedeutet, dass er nur auf das konkrete Element zeigt.
Diese Unterscheidung ist wichtig, da CSS-Selektoren auch so gebaut werden können, dass sie auf mehrere passende
Elemente zeigen.
Das Aufbauen des CSS-Selektors ist der "Copy selector"-Funktionalität der Entwicklerkonsole des Chrome-Browsers
nachempfunden.
Im Grundprinzip traversiert man ausgehend von dem Startelement über die Vater-Beziehung in der
DOM-Hierarchie Richtung Wurzel und speichert sich zu jedem Level die Information, das wievielte Kind das aktuelle
Element ist.
Für das obige Beispiel würde der Text-Selektor wie folgt aussehen: XXX TODO XXX BEISPIEL MIT ID NEHMEN!

\subsubsection{Das Erstellen von Description-Selektoren}
\label{subsubsec:erstellen-von-description-selektoren}

Während unserer Recherchen ist uns häufig eine leicht abgewandelte Form aufgefallen, bei der das gewünschte Attribut
in der Tag-Beschreibung anstatt im Inhalt des Tags stand.
Die Tag-Beschreibung ist nicht Bestandteil der Browser-Visualiserung.

Auch der Description-Selektor besteht wieder aus einem CSS-Selektor, welcher analog zu dem Text-Selektor aufgebaut ist.
Des Weiteren ist jedoch auch der Attributname Bestandteil des Selektors und hilft bei der Auswahl des korrekten
Schlüssel-Wert-Paares bei der Extraktion.

Mit Hilfe dieses Selektors lassen sich auch die Schema.org-Spezifikationen abbilden, insofern der Shop diese befolgt.
Um den Description-Selektor noch etwas flexibler zu machen und weniger starr an die Hierarchie des DOMs anzulehnen,
erstellen wir zusätzlich noch einen zweiten Description Selektor für jedes Vorkommnis.
Dieser zweite Selektor besitzt einen anderen CSS-Selektor, welcher sich nicht an der Hierarchie orientiert, sondern
an allen anderen Schlüssel-Wert-Paaren aus der Beschreibung des Tags.

Die beiden resultierenden Description-Selektoren sehen daher wie folgt aus:
XXX TODO XXX
XXX TODO XXX

\subsubsection{Das Erstellen von Script-Selektoren}
\label{subsubsec:erstellen-von-script-selektoren}

Die Text-Selektoren und die Description-Selektoren beziehen sich beide auf das DOM.
Der Script-Selektor unterscheidet sich dahingehend und bezieht sich auf die auf der Seite enthaltenen
Javascript-Einbindungen.
Viele Content-Management-Systeme verwenden Javascript für die Abspeicherung der Produktinformationen.
Auch JSON-LD, ein ähnlicher Standard wie Schema.org, verwendet spezielle Javascript-Tags für das Markieren von
Produktinformationen.
Dies macht es für uns sehr relevant, auch für diese Fälle ein Schema für den Selektoraufbau zu entwickeln.
Der Script-Selektor ist etwas komplizierter als die bereits vorgestellten Lösungen.

Bereits bei dem Text-Selektor als auch beim Description-Selektor haben wir festgestellt, dass die Hierarchie
innerhalb des HTML-Dokumentes sehr hilfreich ist.
Wir haben daher den Ansatz übernommen und erzeugen innerhalb des Javascript ebenfalls eine Hierarchie.
Der Script-Selektor besteht daher aus drei Teilen:
\begin{enumerate}
    \item dem CSS-Selektor, welcher innerhalb des DOMs zum entsprechenden Script-Element navigiert
    \item dem Block-Path, welcher innerhalb des Javascriptes zum korrespondierenden Block navigiert
    \item dem JsonPath, welcher ein XPath ähnlicher Pfad zu dem gewünschten Wert ist
\end{enumerate}

Der CSS-Selektor wird wieder analog zu dem Text-Selektor aufgebaut.

Der Beginn eines Blockes wird durch das Startsymbol \{ und das Ende durch das Endsymbol \} markiert, wobei eine
öffnende Klammer eine schließende Klammer verlangt und nur Blöcke betrachtet werden, bei denen die Anzahl der
öffnenden Klammern der der Schließenden entspricht.
Ein Block stellt zusammengefasst einen Codeschnipsel dar, der als JSON-Objekt interpretiert werden könnte.

Der Pfad wird letztendlich durch eine Tiefensuche durch die Blöcke des Javascript erstellt, was es ermöglicht auch
verschachtelte Konstrukte abzubilden.

Der JsonPath dient im letzten Schritt dazu, in dem interpretierten JSON-Objekt zu navigieren, um den gesuchten Wert
zurückzugeben.
JSON-Objekte bestehen aus weiteren Objekten, Arrays oder Attributen.
Für die Generierung des JSONPaths wird daher ebenfalls eine Tiefensuche innerhalb des Blocks angewandt.

Schlussendlich entsteht für das obige Beispiel folgender Script-Selektor:
XXX TODO XXX

\subsection{Eine generische Verbesserung aller Selektoren/ Die Trimming-Funktion}
\label{subsec:generische-verbesserung}

Für die Erstellung der obigen Selektoren sind wir bisher davon ausgegangen, dass der gesuchte Wert das einzige ist,
was in dem entsprechenden Feld enthalten ist.
Dies ist tatsächlich nur selten der Fall, so dass wir eine generische Verbesserung für alle Selektoren eingebaut haben.
Diese Verbesserung besteht in einer Art Trimming-Funktion, welche eine bestimmte Anzahl von Zeichen links und rechts
entfernt.
Somit werden auch Fälle wie in Abbildung X abgedeckt.
Der daraus resultierende Selektor sieht dadurch wie folgt aus:
XXX TODO XXX

\subsection{Die Bewertungsfunktion}
\label{subsec:bewertungsfunktion}

Mit der Vielfalt an Selektorenarten und der gesteigerten Flexibilität durch die Trimming-Funktion werden sehr viele
Selektoren erstellt.
Durch die gesteigerte Flexibilität fügt man jedoch auch ein "Rauschen" zu den Selektoren hinzu.
Das bedeutet, dass nun auch Selektoren erzeugt werden, welche nur selten vorkommen.
Sie werden für ein Angebot passend erzeugt, sind jedoch für die restlichen nicht korrekt und produzieren falsche
Ergebnisse.
Um diesen Umstand entgegenzuwirken, wurde eine Bewertungsfunktion eingeführt, welche nach dem Generieren aller
Selektoren ausgeführt wird.
Dazu wird erneut über alle von idealo geladenen Angebotsinformationen iteriert und für jeden Selektor bestimmt, wie
oft dieser ein richtiges Ergebnis extrahiert hat.
Richtig bedeutet in diesem Kontext, dass der extrahierte Wert genau dem Wert aus den Angebotsinformationen entspricht.
Dabei wird für jeden Match der Score erhöht und für jeden Mismatch der Score verringert.
Eine Ausnahme bildet jedoch der leere String "".
In diesem Fall belassen wir den Score so wie er ist.
Wir haben uns dazu entschieden, da wir bei dem leeren String erkennen können, dass die Regel nichts produziert hat.
Wenn die Regel jedoch etwas Falsches zurückliefert, können wir uns später nicht mehr sicher sein, ob dies schlichtweg
falsch, oder etwas Neues ist.
Abschließend wird dieser Score normalisiert, das heißt er wird auf eine Skala von 0 bis 1 abgebildet, indem durch die
Anzahl der Vorkommnisse der einzelnen Attribute in den Angeboten dividiert wird.
Der Score 1 ist hierbei besser und bedeutet, dass in allen Fällen der richtige Wert extrahiert wurde.

Nun verfügt jeder Score über eine genaue Metrik, wie gut der Selektor ist.
Um das Rauschen zu minimieren, werden nun alle Selektoren, deren normalisierter Score sich unter einem bestimmten
Schwellwert befindet verworfen.
Alle übrigen Selektoren werden in geordnet pro Produktattribut in der shop-spezifischen Regel gespeichert.

Die Parser-Komponente verwendet den Score ebenfalls für das Evaluieren des besten Extraktionskandidaten.
Dazu wird nach allen gefunden Werten gruppiert und die normalisierten Scores aufaddiert.
Der Wert mit dem höchsten resultierenden Score wird zurückgegeben.