\section{Extraktion produktspezifischer Daten}
\label{sec:extraktion-produktspezifischer-daten}

Das Extrahieren der Informationen ist ein sehr wichtiger und kritischer Schritt, da die Genauigkeit die Ergebnisse
des Matchers und somit auch des Gesamtergebnisses stark beeinflusst.
Fehlerhafte Informationen könnten das Ergebnis so stark verfälschen, dass die Aussagekraft des Gesamtergebnisses des
Systems in Frage gestellt werden könnte.

Die größte Schwierigkeit bei der Extraktion der produktspezifischen Daten besteht darin, dass der Computer nicht
weiß, an welchen Stellen er die gewünschten Informationen findet.
Um dieses Problem zu lösen, unterscheiden wir zwischen zwei Herangehensweisen, welche im nachfolgenden Kapitel
erläutert werden.

\subsection{Die möglichen Herangehensweisen}
\label{subsec:herangehensweisen}

\begin{comment}
    schema.org
\end{comment}

Es gibt grundsätzlich zwei Möglichkeiten, wie man das Problem der Datenextraktion angehen kann.
Vordergründig geht es hierbei um die Frage, wie man aus den unterschiedlich strukturierten Shops Informationen
extrahiert.

Das von uns zu lösende Problem gibt es nicht zum ersten Mal.
Bereits Google hat sich mit der Thematik befasst, da es bei den Suchergebnissen nähere Informationen zu den Angeboten
anzeigen möchte.
Im Zuge dessen hat Google gemeinsam mit den Suchmaschinen von Microsoft, Yahoo und Yandex den Schema.org - Standard für
die strukturierte Datenangabe im Internet entwickelt.
Laut einer Schätzung von idealo verwenden rund 40\% der Shops diesen Standard.
Diese Herangehensweise bezeichnen wir als shop-unspezifischen Ansatz, da wir eine generische Regel verwenden können,
um die standardisierten Informationen zu erfassen.
Dieser Option hat den klaren Vorteil, dass sie recht schnell und einfach umzusetzen ist.

Alternativ zum shop-unspezifischen Ansatz, gibt es die shop-spezifische Herangehensweise.
Bei dem shop-spezifischen Ansatz geht es darum, für jeden Shop gesonderte Spezifikationen zu erstellen.
Die Regeln des shop-unspezifischen Ansatzes würden eine Untermenge des shop-spezifischen Ansatzes darstellen.
Die Umsetzung dieser Variante wäre insgesamt anspruchsvoller.
Insgesamt gehen wir jedoch davon aus, sowohl die Extraktionsrate als auch die Präzision des Parsers im Vergleich zu
dem shop-unspezifischen Ansatz zu erhöhen und somit bessere Daten für das Vergleichen zu erhalten.

Zu Beginn haben wir erste Versuche basierend auf dem Schema.org-Standard unternommen.
Leider haben wir schnell feststellen müssen, dass die Spezifikation oft nicht richtig eingehalten wurden, was die
Datenqualität stark verringert hat.
Auch bei anderen Standards, welche bei der Strukturierung von Produktdaten im Internet helfen sollen wie zum Beispiel
JSON-LD (W3C) und das Open-Graph-Protokoll (Facebook) konnten wir die gleiche Beobachtung machen.
Wir haben uns deshalb gegen diesen Ansatz entschieden.

Im nachfolgenden wird darauf eingegangen, wie wir die shop-spezifische Methode umgesetzt und in den Ablauf der
Extraktionsprozesses integriert haben.

\subsection{Annahmen und Grundidee}
\label{subsec:annahmen-und-grundidee}

Wir folgen der Annahme, dass jeder Shop ein CMS (Content Management System) zur Verwaltung seiner Angebote verwendet.
Des Weiteren gehen wir davon aus, dass sich durch die Verwendung eines CMS die Struktur der Angebote ähnelt und diese
erlernt werden kann.
Des Weiteren nehmen wir an, dass idealo aufgrund der Vertragsvereinbarungen für den zu untersuchenden Shops bereits
eine gewisse Menge an Angeboten besitzt und die Produktattribute nicht manipuliert wurden.

Der grobe Ablauf der shop-spezifischen Datenextraktion kann in zwei Phasen untergliedert werden:
dem Generieren der shop-spezifischen Extraktionsregeln und dem Anwenden dieser auf die von der Crawler-Komponente
heruntergeladenen Seiten.
Unter der Regel kann man sich zunächst so etwas wie eine Wegbeschreibung durch das HTML-Dokument zu dem gewünschten
Element vorstellen.
Die Grundidee der ersten Phase besteht darin, dass die Regeln, welche für das Extrahieren benötigt werden mit
Hilfe der Daten von idealo angelernt werden können.
Die generierten Regeln werden nun in der zweiten Phase angewendet, sodass für jedes Attribut genau ein Wert
zugeordnet wird.
Für jede gecrawlte Seite werden die extrahierten Werte für jedes Attribut gemeinsam abgespeichert.

Die Logik der beiden Komponenten spiegelt sich auch in der genaueren Architektur des Parsers wieder.
Dieser besteht aus der Shop Rules Generator (SRG) Komponente und der Parser Komponente.
Im nachfolgenden werde ich auf die Funktionsweise der beiden Komponenten einzeln eingehen.

\subsection{Die Funktionsweise der Parser-Komponente}
\label{subsec:funktionsweise-parser}

Die Parser-Komponente lauscht auf eine Queue, über die sie die vom Crawler heruntergeladenen Seiten erhält.
Der Crawler sendet zu jeder Seite auch die Webadresse und die Identifikationsnummer des zugehörigen Shops mit.
Nachdem der Parser etwas aus der Queue empfangen hat, fragt er den SRG nach Regeln für den entsprechenden Shop.
Sollten die Regeln noch nicht existieren, so wartet er solange, bis der SRG diese erstellt hat.
Sobald dieser die Regeln als Antwort sendet, wendet der Parser diese Regeln auf der gecrawlten Seite an.
Der extrahierte Preis und die Bilderlinks werden von dem Parser normalisiert, ehe sie gemeinsam mit den anderen
Attributen als ParsedOffer in einer Datenbank persistiert werden.
Der Matcher greift später auf diese Datenbank für den Vergleich zu.

Um den nichtfunktionalen Anforderungen, insbesondere der Skalierbarkeit und Parallelität zu genügen, arbeitet die
Parser-Komponente mit mehreren Threads und kann somit mehrere gecrawlte Seiten gleichzeitig verarbeiten.
Zudem wurden Cache-Mechanismen verwendet, um die Anfragen an den SRG zu minimieren.


\subsection{Die Funktionsweise des SRG}
\label{subsec:funktionsweise-srg}

Die Shop-Rules-Generator-Komponente wartet über eine REST-Schnittstelle auf eine Anfrage und gibt für den angegebenen
Shop die Regeln für die Extraktion zurück.
Wenn die Regel für den Shop bereits in der Datenbank existiert, wird diese vom SRG zurückgegeben.
Andernfalls wird der Generierungsprozess für den Shop gestartet.

Während des Generierungsprozesses durchläuft der SRG mehrere Phasen.
Zuerst lädt der SRG eine bestimmte Anzahl von Angeboten von der idealo-Datenbank.
Dazu nutzt er die von idealo zur Verfügung gestellte IdealoBridge - dies ist eine API, welche den Zugriff über
eine REST-Schnittstelle kapselt.
Zu jedem dieser Angebote liegen die Adresse für das Angebot, sowie die  Informationen über die in
Kapitel~\ref{subsec:technische-anforderungen-parser} genannten Attribute vor.
Für jedes Angebot ruft der SRG die verlinkte Angebotsseite auf und speichert das HTML-Dokument dieser.
Wir wissen nun, dass es sich bei dem HTML-Dokument um das bestimmte Angebot handelt und können dies ausnutzen.
Dazu sucht der SRG den Wert aller einzelnen Produktattribute und erstellt für jeden Fundort eine Regel, welche auf
das entsprechende Element zeigt.
Nachdem er dies für alle heruntergeladenen Angebote gemacht hat, kann er aus den gesammelten Regeln eine
finale Regelmenge bestimmen.
Diese Regelmenge wird in einer Datenbank gespeichert und bei zukünftigen Anfragen direkt zurückgegeben.